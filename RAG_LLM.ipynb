! pip install langchain_community langchain-huggingface langchain -q

! pip install faiss-cpu -q

import faiss
from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_huggingface import HuggingFaceEndpoint
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import numpy as np
import os

-- for token use www.huggingface.co and create new token
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your_hf_token_here"

with open('YOUR_TXT_FILE.txt','r',encoding='utf-8') as file_reader:
  text = file_reader.read()
  file_reader.close()

text_splitter = CharacterTextSplitter(chunk_size=1000,chunk_overlap=200)
chunks = text_splitter.split_text(text)

hf_embeddings = HuggingFaceEmbeddings(
    model_name = "sentence-transformers/all-mpnet-base-v2",
    model_kwargs = {'device':'cuda'},
    encode_kwargs = {'normalize_embeddings':False}
)

embeddings = hf_embeddings.embed_documents(chunks)
dimension = len(embeddings[0])

quantizer = faiss.IndexFlatL2(dimension)
index = faiss.IndexIVFFlat(quantizer, dimension, 100)
index.train(np.array(embeddings).astype('float32'))
index.add(np.array(embeddings).astype('float32'))



documents = [Document(page_content=chunk) for chunk in chunks]
docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(documents)})
index_to_docstore_id = {i: str(i) for i in range(len(documents))}

vector_store = FAISS(embedding_function=hf_embeddings.embed_query,index=index, docstore=docstore, index_to_docstore_id=index_to_docstore_id)

vector_store.save_local('vector_search')


llm = HuggingFaceEndpoint(
    repo_id = "mistralai/Mistral-7B-Instruct-v0.2",
    task = "text-generation",
    temperature=0.6
)


             
vector_search = FAISS.load_local("/content/vector_search/",embeddings=hf_embeddings, allow_dangerous_deserialization=True)
query = "tap something"

qa = RetrievalQA.from_chain_type(llm=llm,retriever=vector_search.as_retriever())
answer = qa({'query':query})
print(answer)
             
